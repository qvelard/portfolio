"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[448],{9155:function(e,t,n){n.d(t,{C:function(){return o}});var a=n(7437);n(2265);var i=n(9213),s=n(9311);let r=(0,i.j)("inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2",{variants:{variant:{default:"border-transparent bg-primary text-primary-foreground hover:bg-primary/80",secondary:"border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80",destructive:"border-transparent bg-destructive text-destructive-foreground hover:bg-destructive/80",outline:"text-foreground"}},defaultVariants:{variant:"default"}});function o(e){let{className:t,variant:n,...i}=e;return(0,a.jsx)("div",{className:(0,s.cn)(r({variant:n}),t),...i})}},3023:function(e,t,n){n.d(t,{z:function(){return l}});var a=n(7437),i=n(2265),s=n(4949),r=n(9213),o=n(9311);let c=(0,r.j)("inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50",{variants:{variant:{default:"bg-primary text-primary-foreground hover:bg-primary/90",destructive:"bg-destructive text-destructive-foreground hover:bg-destructive/90",outline:"border border-input bg-background hover:bg-accent hover:text-accent-foreground",secondary:"bg-secondary text-secondary-foreground hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-10 px-4 py-2",sm:"h-9 rounded-md px-3",lg:"h-11 rounded-md px-8",icon:"h-10 w-10"}},defaultVariants:{variant:"default",size:"default"}}),l=i.forwardRef((e,t)=>{let{className:n,variant:i,size:r,asChild:l=!1,...d}=e,m=l?s.g7:"button";return(0,a.jsx)(m,{className:(0,o.cn)(c({variant:i,size:r,className:n})),ref:t,...d})});l.displayName="Button"},6110:function(e,t,n){n.d(t,{Zb:function(){return r},aY:function(){return d}});var a=n(7437),i=n(2265),s=n(9311);let r=i.forwardRef((e,t)=>{let{className:n,...i}=e;return(0,a.jsx)("div",{ref:t,className:(0,s.cn)("rounded-lg border bg-card text-card-foreground shadow-sm",n),...i})});r.displayName="Card";let o=i.forwardRef((e,t)=>{let{className:n,...i}=e;return(0,a.jsx)("div",{ref:t,className:(0,s.cn)("flex flex-col space-y-1.5 p-6",n),...i})});o.displayName="CardHeader";let c=i.forwardRef((e,t)=>{let{className:n,...i}=e;return(0,a.jsx)("h3",{ref:t,className:(0,s.cn)("text-2xl font-semibold leading-none tracking-tight",n),...i})});c.displayName="CardTitle";let l=i.forwardRef((e,t)=>{let{className:n,...i}=e;return(0,a.jsx)("p",{ref:t,className:(0,s.cn)("text-sm text-muted-foreground",n),...i})});l.displayName="CardDescription";let d=i.forwardRef((e,t)=>{let{className:n,...i}=e;return(0,a.jsx)("div",{ref:t,className:(0,s.cn)("p-6 pt-0",n),...i})});d.displayName="CardContent";let m=i.forwardRef((e,t)=>{let{className:n,...i}=e;return(0,a.jsx)("div",{ref:t,className:(0,s.cn)("flex items-center p-6 pt-0",n),...i})});m.displayName="CardFooter"},7145:function(e,t,n){n.d(t,{MS:function(){return s},aS:function(){return r},bd:function(){return i}});let a=[{id:"1",title:"Biomolecule Generation with Anti-oxydant and Anti-inflammatory properties",slug:"biomolecule-generation",description:'his Project, authored by Quentin Velard and Salma Bouaouda under Guenael Cabanes at \xc9cole des Mines de Nancy, leverages deep learning to generate biomolecules with anti-inflammatory (AI) and antioxidant (AO) properties. This work supports the "Biomolecules 4 Bioeconomy" framework, with potential applications in pharmaceuticals, agrochemicals, and cosmetics.',longDescription:"\n      Built a full-featured e-commerce dashboard that provides store owners with comprehensive insights into their business. \n      The application features real-time sales analytics, inventory management, customer insights, and order processing capabilities.\n      \n      Key challenges included implementing real-time data synchronization across multiple users, optimizing performance for large datasets, \n      and creating an intuitive interface that works seamlessly across desktop and mobile devices.\n      \n      The solution leverages React with TypeScript for type safety, Socket.io for real-time updates, and a microservices architecture \n      to ensure scalability and maintainability.\n    ",image:"/v1/biomol.webp",images:["https://images.pexels.com/photos/590016/pexels-photo-590016.jpeg?auto=compress&cs=tinysrgb&w=800","https://images.pexels.com/photos/265087/pexels-photo-265087.jpeg?auto=compress&cs=tinysrgb&w=800","https://images.pexels.com/photos/669996/pexels-photo-669996.jpeg?auto=compress&cs=tinysrgb&w=800"],technologies:["React","TypeScript","Node.js","PostgreSQL","Socket.io","Tailwind CSS"],category:"web-app",github:"https://github.com/alexjohnson/ecommerce-dashboard",demo:"https://ecommerce-dashboard-demo.vercel.app",date:"2024",role:"Full-Stack Developer",featured:!0},{id:"2",title:"Project QILLER",slug:"project-qiller",description:'This Project, authored by Quentin Velard and Salma Bouaouda under Guenael Cabanes at \xc9cole des Mines de Nancy, leverages deep learning to generate biomolecules with anti-inflammatory (AI) and antioxidant (AO) properties. This work supports the "Biomolecules 4 Bioeconomy" framework, with potential applications in pharmaceuticals, agrochemicals, and cosmetics.',longDescription:"Catastrophic forgetting is a key problem this paper addresses. In both classical and quantum machine learning, this happens when a model learns new information but loses its ability to remember older knowledge—like forgetting how to ride a bike after learning to drive a car. The authors show this issue exists in variational quantum algorithms (VQAs), a type of quantum machine learning model, and aim to fix it.\n\nQILLER combines several cool techniques to solve this: representation learning, knowledge distillation, and exemplar memory. These work together to help the model learn new tasks incrementally while holding onto past knowledge, all within a quantum computing framework.\n\nThe goal is lifelong learning, inspired by how humans can keep learning new skills without forgetting old ones. The authors test their method on datasets like MNIST (handwritten digits), FMNIST (fashion items), KMNIST (Japanese characters), and CIFAR-10 (color images), showing it outperforms other approaches.\n\n\uD83D\uDDDD️ Key Concepts Explained\nCatastrophic Forgetting: Imagine training a quantum classifier to recognize quantum phases of matter (a physics task), and it works great. Then, you train it on handwritten digits (like 0s and 1s). Suddenly, it forgets the physics task entirely! That's catastrophic forgetting, and the paper gives this exact example from prior research to highlight the issue in VQAs.\n\nRepresentation Learning: This is about transforming messy data (like images) into a simpler, more useful form—like summarizing a book into key points. Here, it's done with a quantum autoencoder, which compresses classical data into a quantum state that's easier for the model to work with.\n\nKnowledge Distillation: Think of it as a student learning from a teacher. The \"teacher\" is the model's past knowledge, and the \"student\" is the updated model. It uses a special loss function to ensure the new model mimics what the old one knew, preventing forgetting.\n\nExemplar Memory: This acts like a scrapbook of important examples. It stores a small, representative set of samples from old tasks (picked using a method called herding selection) so the model can revisit them and remember past lessons.\n\n\uD83D\uDD27 How QILLER Works\nTwo-Part Architecture: QILLER splits into a feature extractor and a classifier. The feature extractor uses a quantum autoencoder (QAE) to turn classical data into quantum features—a 64-dimensional vector—trained once with supervised contrastive loss and then frozen. The classifier, a Variational Quantum Classifier (VQC), predicts classes using these features.\n\nIncremental Learning Steps: It starts by training on the first task with cross-entropy loss. As new tasks come in, the VQC updates using a mix of cross-entropy loss (for new classes) and distillation loss (to keep old knowledge), referencing past outputs stored in exemplar memory. After each step, the memory updates with new representative samples.\n\nLoss Function Magic: The loss combines cross-entropy for new learning and distillation to retain old knowledge, with a temperature parameter (T=2) balancing it. It's optimized with COBYLA, a gradient-free method suited for quantum circuits.\n\n\uD83D\uDCC8 Experimental Results\nImpressive Numbers: QILLER was tested on MNIST, FMNIST, KMNIST, and CIFAR-10. On MNIST, it hit 29% accuracy for 10 classes (1-class-per-step) versus 9-27% for rivals like SCL-VQC, iCaRL-VQC, and TL-VQC. On FMNIST, it scored 28%, beating TL-VQC (19%). KMNIST saw 24%, topping others (12-15%). CIFAR-10 achieved 16%, outperforming TL-VQC (9%).\n\nReal Quantum Hardware: Tested on Amazon Braket's IonQ Harmony (11 qubits) and simulators, it proves practical even with limits like resizing images to 16x16 pixels.\n\n⚠️ Not Perfect Yet: Accuracies drop as classes grow due to quantum hardware constraints—still a work in progress!\n\n\uD83C\uDF1F Why It's Awesome\nReal-World Ready: QILLER tackles continuous data streams (think smart cities or social media) where new info keeps coming, unlike older methods that struggle with scalability or forget too much.\n\nEfficient Design: It uses fewer qubits and discards old VQCs after training, saving quantum resources while still learning effectively.\n\nBeats the Competition: Outperforms methods like iCaRL-VQC (memory-heavy) and SCL-VQC (less adaptive) by balancing feature extraction and classification in a quantum way.\n\n\uD83D\uDEA7 Challenges and Future\nHardware Limits: Quantum hardware keeps accuracies low for many classes—think 16-29% for 10 classes versus classical models' 90%+. Noise and small qubit counts are the culprits.\n\nFuture Potential: As quantum tech improves, QILLER could scale to bigger, more complex tasks, making quantum lifelong learning a reality.\n\nFor more detailed insights, you can download the full project report below:",image:"/v1/quiller.webp",images:["https://images.pexels.com/photos/196644/pexels-photo-196644.jpeg?auto=compress&cs=tinysrgb&w=800","https://images.pexels.com/photos/267350/pexels-photo-267350.jpeg?auto=compress&cs=tinysrgb&w=800","https://images.pexels.com/photos/177598/pexels-photo-177598.jpeg?auto=compress&cs=tinysrgb&w=800"],technologies:["React Native","TypeScript","Firebase","Redux","Expo"],category:"mobile",github:"https://github.com/qvelard/task-management-app",demo:"https://apps.apple.com/app/taskflow",date:"2023",role:"Mobile Developer",featured:!0},{id:"3",title:"F1 RAG Chatbot: Formula 1 Retrieval-Augmented Generation System",slug:"f1-rag-chatbot",description:"Le F1 RAG Chatbot combine une base de connaissances vectorielle, un pipeline d'embedddings, un syst\xe8me de retrieval avanc\xe9 et GPT-4 pour fournir des r\xe9ponses pr\xe9cises et actualis\xe9es sur la Formule 1.",longDescription:'1. Vector Database (Astra DB)\nAt the core of the system is a vector database that stores Formula 1 documents as high-dimensional embeddings. Each document is transformed into a 1536-dimensional vector that captures its semantic meaning, enabling similarity-based search beyond simple keyword matching.\n\nThe database schema includes not just the embedding vectors but also metadata like source, date, and category, allowing for comprehensive retrieval and attribution of information. This foundation ensures the chatbot can access specific F1 knowledge that may not be present in general LLM training data.\n\n// Example document schema in Astra DB\n{\n  id: "doc-124578",\n  text: "The 2025 Formula 1 season will feature 24 races, including the new Madrid Grand Prix...",\n  source: "Formula1.com",\n  date: "2024-06-10",\n  category: "calendar",\n  $vector: [0.023, -0.112, 0.438, ...] // 1536-dim vector\n}\n\n2. Embedding Generation Pipeline\nThe system uses OpenAI\'s text-embedding-3-small model to convert both F1 documents and user queries into the same vector space. This common mathematical representation enables semantic matching where queries find relevant information even when using different terminology.\n\nDuring indexing, documents are processed in batches to optimize API usage, with each document carefully segmented to maintain context while fitting within the model\'s token limits. For live queries, the embedding process transforms user questions into the same vector format to enable similarity search.\n\n// Converting user queries to embeddings\nasync function generateEmbedding(text) {\n  const response = await openai.embeddings.create({\n    model: "text-embedding-3-small",\n    input: text,\n    dimensions: 1536\n  });\n  return response.data[0].embedding;\n}\n\n3. Context Retrieval System\nThe retrieval system performs vector similarity search to find content relevant to user questions. Rather than simple retrieval, the system employs sophisticated ranking and filtering:\n\nSimilarity Threshold Filtering - Only documents above 0.70 cosine similarity are considered\nSource Authority Weighting - Official F1 sources receive priority in retrieval\nRecency Prioritization - More recent documents are favored for time-sensitive topics\nContextual Diversity - The system seeks to include varied perspectives when appropriate\nThis carefully tuned retrieval process ensures that the LLM receives the most relevant, accurate, and up-to-date Formula 1 information as context for generating responses.\n\n4. Language Model Integration\nThe system integrates with GPT-4 through a carefully structured prompt architecture that consists of:\n\nA system prompt establishing the AI as an F1 expert with specific guidelines\nRetrieved context from the vector database formatted with source attribution\nConversation history to maintain dialogue coherence\nThe current user question\nResponse streaming is implemented for enhanced user experience, delivering content as it\'s generated rather than waiting for complete responses. Parameters like temperature (0.7) are tuned to balance creativity with factual accuracy in F1 discussions.\n\n// Structured prompt for GPT-4\nconst promptMessages = [\n  { role: "system", content: "You are F1GPT, an expert on Formula 1 racing..." },\n  { role: "system", content: `CONTEXT INFORMATION:\n${retrievedContext}` },\n  ...conversationHistory,\n  { role: "user", content: userQuestion }\n];\n\n5. Data Flow Architecture\nThe complete RAG pipeline follows these steps during each interaction:\n\nQuery Processing - User\'s F1 question is received by the Next.js API route\nEmbedding Generation - The question is transformed into a vector embedding\nSimilarity Search - The vector database is queried for relevant F1 content\nContext Assembly - Retrieved documents are formatted with source information\nPrompt Construction - A structured prompt combines the context, conversation history, and user question\nResponse Generation - GPT-4 generates a response grounded in the provided context\nStreaming Delivery - The response is streamed token-by-token to the user interface\nThis architecture ensures that responses are informed by the latest F1 information while maintaining the natural conversational abilities of advanced language models.\n\n6. Frontend Implementation\nThe React-based frontend creates an intuitive chat experience using the Vercel AI SDK. Key features include:\n\nReal-time Response Streaming - Text appears progressively for immediate feedback\nContext-aware Suggestions - Follow-up questions are generated based on conversation context\nMobile-responsive Design - Layout adapts to different screen sizes with optimized interactions\nF1 Theming - Visual design elements reflect Formula 1 branding and aesthetics\nThe user interface manages state using React hooks and provides smooth transitions between conversation states, with automatic scrolling to follow new messages.\n\n7. Knowledge Management\nA sophisticated document processing pipeline maintains the F1 knowledge base:\n\nContent Curation - F1 articles, regulations, statistics, and historical data are sourced from authoritative sites\nDocument Processing - Content is cleaned, segmented, and prepared for embedding\nBatched Indexing - Documents are embedded and stored in batches of 20 for efficiency\nRegular Updates - The knowledge base is refreshed with new F1 content after races and announcements\nThis systematic approach to knowledge management ensures that the chatbot has access to comprehensive and current Formula 1 information.\n\n8. Technical Optimizations\nThe implementation includes several performance optimizations:\n\nVector Search Efficiency - Approximate nearest neighbor algorithms accelerate similarity search\nResponse Caching - Common F1 questions are cached to reduce embedding and retrieval costs\nIncremental Loading - UI components load progressively to improve perceived performance\nError Handling - Robust fallback mechanisms ensure service continuity during API disruptions\nResource Conservation - Careful management of API calls to balance cost and performance\nThese optimizations create a responsive, reliable chatbot experience even under high load conditions.\n\n9. Deployment Infrastructure\nThe F1 RAG Chatbot is deployed on Railway with containerization for consistent environments:\n\nDocker Containerization - Ensures consistent runtime across environments\nEnvironment Variable Management - Secures API keys and configuration\nAuto-scaling - Adapts to traffic patterns for cost efficiency\nMonitoring - Tracks usage patterns and performance metrics\nThis infrastructure provides a stable foundation for delivering the F1 chatbot to global users with minimal latency and high reliability.\n\n10. Conclusion and Future Enhancements\nThe F1 RAG architecture delivers a significant improvement in accuracy and relevance for Formula 1 information compared to general-purpose chatbots. Future enhancements could include:\n\nMulti-modal Support - Visual recognition of F1 car components and track layouts\nLive Data Integration - Real-time race information during Grand Prix weekends\nPersonalization - User preference tracking for favorite teams and drivers\nAdvanced Analytics - F1 performance predictions and statistical analysis\nBy combining vector search with generative AI, this architecture creates a specialized knowledge system that delivers the kind of detailed, accurate F1 information that passionate fans demand.\n\nWant to give it a try? You can access the F1 RAG Chatbot app at the link below.',image:"/v1/simple-rag.webp",images:["https://images.pexels.com/photos/1181244/pexels-photo-1181244.jpeg?auto=compress&cs=tinysrgb&w=800","https://images.pexels.com/photos/270348/pexels-photo-270348.jpeg?auto=compress&cs=tinysrgb&w=800","https://images.pexels.com/photos/574071/pexels-photo-574071.jpeg?auto=compress&cs=tinysrgb&w=800"],technologies:["Next.js","TypeScript","OpenAI","Astra DB","Langchain.js","Vercel AI SDK","Railway"],category:"tool",github:"https://github.com/qvelard/f1-rag-chatbot",demo:"#",date:"2024",role:"Lead Developer",featured:!1}],i=[{id:"all",name:"All Projects"},{id:"web-app",name:"Web Apps"},{id:"mobile",name:"Mobile"},{id:"tool",name:"Tools"},{id:"ml",name:"Machine Learning"}];function s(){return a.filter(e=>e.featured)}function r(e){return"all"===e?a:a.filter(t=>t.category===e)}},9311:function(e,t,n){n.d(t,{cn:function(){return s}});var a=n(7042),i=n(4769);function s(){for(var e=arguments.length,t=Array(e),n=0;n<e;n++)t[n]=arguments[n];return(0,i.m6)((0,a.W)(t))}}}]);