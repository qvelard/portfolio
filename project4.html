<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 13: Water Reservoir Management with Q-Learning</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        .container {
            width: 80%;
            margin: 20px auto;
            padding: 20px;
            background: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
        }
        h1, h2, h3, h4 {
            color: #333;
        }
        h1 {
            font-size: 2em;
            margin-bottom: 10px;
        }
        h2 {
            font-size: 1.5em;
            margin-top: 20px;
            border-bottom: 2px solid #333;
            padding-bottom: 5px;
        }
        h3 {
            font-size: 1.2em;
            margin-top: 15px;
        }
        h4 {
            font-size: 1em;
            margin: 10px 0 5px;
            color: #555;
        }
        p {
            margin: 10px 0;
        }
        ul {
            list-style-type: disc;
            padding-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
        .image-desc {
            background: #e9e9e9;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }
        .image-desc p {
            margin: 5px 0;
        }
        footer {
            text-align: center;
            margin-top: 20px;
            font-size: 0.9em;
            color: #666;
        }
        a {
            color: #0066cc;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Water Reservoir Management with Q-Learning üíß</h1>
        <p>This project, authored by Arthur Tornqvist and Quentin Velard, explores the use of Q-Learning, a reinforcement learning technique, to optimize water reservoir management. The objective is to maintain water levels within acceptable limits by deciding whether to conserve or release water, effectively balancing inflow and outflow in a dynamic environment.</p>
        <p>The problem is modeled as a Markov Decision Process (MDP), where an agent learns optimal actions based on states (water levels), actions (release or conserve water), and rewards (positive for maintaining optimal levels, negative for extremes). An Œµ-greedy policy is employed to balance exploration and exploitation, with simulations conducted to evaluate and refine the approach.</p>

        <h2>üóùÔ∏è Key Concepts</h2>
        <ul>
            <li><strong>Problem Modeling</strong>: The reservoir system is framed as an MDP with states representing water levels, actions involving releasing 0, 1, or 2 units of water, transition probabilities based on random inflows and chosen outflows, and a reward structure that penalizes overflow or depletion while rewarding optimal levels.</li>
            <li><strong>Q-Learning</strong>: A model-free reinforcement learning algorithm that iteratively updates a Q-table to estimate the value of actions in given states, aiming to maximize cumulative future rewards.</li>
            <li><strong>Œµ-Greedy Policy</strong>: This policy balances exploration (random actions) and exploitation (best-known actions) using a parameter Œµ, which can decay over time to favor exploitation as learning progresses.</li>
            <li><strong>Hyperparameter Optimization</strong>: Parameters such as learning rate (Œ±), discount factor (Œ≥), and exploration rate (Œµ) are tuned, often with decay strategies, using techniques like Bayesian optimization to enhance performance.</li>
        </ul>

        <h2>üîß How It Works</h2>
        <ul>
            <li><strong>State and Action Definition</strong>: The state \( s(t) \) is the water level at time \( t \), updated as \( s(t+1) = s(t) + Q_{\text{in}}(t) - Q_{\text{out}}(t) \), where \( Q_{\text{in}}(t) \) is the inflow and \( Q_{\text{out}}(t) \) is the outflow (actions: release 0, 1, or 2 units).</li>
            <li><strong>Reward Function</strong>: A reward of +1 is given when the water level is near a reference level \( s_{\text{ref}} \), -1 for levels too high or low but within bounds, and -10 for exceeding acceptable limits (overflow or depletion).</li>
            <li><strong>Q-Learning Update</strong>: The Q-table is updated using the formula:<br>
                \( Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] \),<br>
                where \( \alpha \) is the learning rate, \( \gamma \) is the discount factor, \( r \) is the reward, and \( s' \) is the next state.</li>
        </ul>

        <h2>üìà Experimental Results</h2>
        <p><strong>Simulations</strong>: The reservoir‚Äôs water level was simulated over multiple episodes, with performance tracked via moving averages of rewards. Various hyperparameter settings were tested, including learning rate (Œ±), exploration rate (Œµ), and discount factor (Œ≥), with and without decay.</p>
        <p><strong>Optimized Hyperparameters</strong>: Bayesian optimization identified effective parameter combinations, such as {Œ±: 0.88, Œµ: 0.44, Œ≥: 0.86}, yielding stable average rewards up to 95 over the last 50 episodes.</p>
        <p><strong>Challenges</strong>: Initial simulations showed reward variance, which was mitigated by implementing decay strategies for Œµ, leading to more consistent performance.</p>

        <h3>Visualizations</h3>
        <div class="image-desc">
            <h4>Q-Table Finale (Heatmap)</h4>
            <p>This heatmap, titled "Q-Table finale," displays the final Q-values for a Q-Learning model after training. It consists of 10 rows (states 0 to 9) and 3 columns (actions 0 to 2), with each cell‚Äôs color representing the Q-value, ranging from -10 (dark purple) to 10 (bright yellow). A color bar on the right indicates the value scale.</p>
            <p><strong>Specific Values</strong>: For example, State 0: [0.77, -4.33, -5.57]; State 4: [10.00, 9.45, 7.84]; State 9: [-1.37, -2.63, 5.61]. The highest values (10.00) occur at (State 4, Action 0), (State 5, Action 1), and (State 6, Action 2), shown in bright yellow, while the lowest (-5.57) is at (State 0, Action 2) in dark purple.</p>
            <p><strong>Observations</strong>: Q-values peak around states 4‚Äì6, suggesting optimal actions, and decline toward states 0 and 9, reflecting less favorable outcomes.</p>
        </div>
        <div class="image-desc">
            <h4>√âvolution du niveau d'eau (Water Level Evolution)</h4>
            <p>This time series graph, titled "√âvolution du niveau d'eau," tracks water levels over 90,000 steps. The x-axis ("√âtapes") ranges from 0 to 90,000, and the y-axis ("Niveau d'eau") from 0 to 10. Vertical blue lines show water level fluctuations, mostly between 2 and 8, with a shaded blue area indicating this range. Occasional spikes to 0 or 10 occur, demonstrating the agent‚Äôs ability to maintain levels within bounds despite variability.</p>
        </div>
        <div class="image-desc">
            <h4>R√©compenses par √©pisode (Rewards per Episode)</h4>
            <p>Multiple graphs titled "R√©compenses par √©pisode" depict raw rewards (orange shaded area) and 50-episode moving averages (blue line) over 1000 episodes. The x-axis ("√âpisode") spans 0 to 1000, and the y-axis ("R√©compense totale") ranges from -20 to 100.</p>
            <p><strong>Trends</strong>: Raw rewards fluctuate widely between -20 and 100, showing high variability. The moving average typically starts low (e.g., 20‚Äì50), rises to 70‚Äì90 by episode 200‚Äì400, and stabilizes, though some graphs show late increases (e.g., from 50 to 80 near episode 1000). This indicates consistent learning despite noisy raw data.</p>
        </div>
        <div class="image-desc">
            <h4>Comparaison des performances pour diff√©rents param√®tres (Performance Comparison)</h4>
            <p>This line graph, titled "Comparaison des performances pour diff√©rents param√®tres," compares average rewards per episode across 1000 episodes for different parameter sets. The x-axis ("√âpisode") ranges from 0 to 1000, and the y-axis ("R√©compense moyenne par √©pisode") from 30 to 90.</p>
            <p><strong>Parameter Sets</strong>: Blue (Œ±=0.1, Œ≥=0.9, Œµ=0.1) peaks at 70‚Äì80; Orange (Œ±=0.2, Œ≥=0.9, Œµ=0.2) stabilizes at 60‚Äì70; Green (Œ±=0.1, Œ≥=0.8, Œµ=0.3) ranges 40‚Äì50. Higher Œ≥ (0.9) consistently yields better, more stable rewards.</p>
        </div>

        <h2>üåü Significance</h2>
        <ul>
            <li><strong>Real-World Application</strong>: This Q-Learning approach can optimize water reservoir management under unpredictable conditions, such as variable rainfall, enhancing resource efficiency.</li>
            <li><strong>Efficient Learning</strong>: By not requiring a predefined environmental model, Q-Learning adapts to complex, dynamic systems effectively.</li>
            <li><strong>Scalability</strong>: Techniques like hyperparameter optimization and Œµ decay ensure the system improves over time, making it versatile for broader applications.</li>
        </ul>

        <h2>üöß Limitations</h2>
        <ul>
            <li><strong>Simplifications</strong>: The model uses discrete actions and simplified inflows, potentially missing real-world complexities like seasonal variations or extreme events.</li>
            <li><strong>Future Potential</strong>: Future enhancements could include continuous action spaces, more nuanced reward functions, or real-time data integration for practical use.</li>
        </ul>

        <h2>Contact Information</h2>
        <p><strong>Address</strong>: Paris, France</p>
        <p><strong>Email</strong>: <a href="mailto:contact@project13.qlearning">contact@project13.qlearning</a></p>
        <p><strong>Social Links</strong>: 
            <a href="https://twitter.com/project13" target="_blank">Twitter</a> | 
            <a href="https://github.com/project13" target="_blank">GitHub</a> | 
            <a href="https://www.linkedin.com/in/project13" target="_blank">LinkedIn</a>
        </p>

        <footer>
            <p>Copyright ¬© Quentin Velard 2025</p>
        </footer>
    </div>
</body>
</html>